# Foundry Cluster Preflight
- name: Cluster Hosts Pre-flight
  hosts:  "{{ groups['kube-node'] }}"
  become: true
  become_method: sudo
  become_user: root
  gather_facts: true
  
  tasks:
    - name: Ping
      ping:
      tags: info

    - name: Yum Update
      raw: yum update -y
      tags: ['prereq']
      when: ansible_distribution == "CentOS"

    - name: Yum Install Epel
      raw: yum install -y epel-release
      tags: ['prereq', 'never']
      when: ansible_distribution == "CentOS"
    
    # Elasticsearch requires a max map count > 262144
    - name: Get /etc/sysctl.conf
      shell: cat /etc/sysctl.conf | grep max_map_count 
      ignore_errors: true
      register: max_map_count
      tags: ['elastic']

    - name: Expand max_map_count 
      shell: |
        cp /etc/sysctl.conf /etc/sysctl.conf.{{ 10000 | random }}.bak
        echo "vm.max_map_count=262144" >> /etc/sysctl.conf
        sysctl -p
      register: map_max_update
      when: 
        - max_map_count.stdout | int < 262144 
        # May need check for it is there and not large enough 
      tags: ['elastic']

    - name: Get sudoers secure_path
      shell: cat /etc/sudoers | grep secure_path
      register: sudoers_path
        
    # Require /usr/local/bin in the sudoers path so 'su' will find kubectl and helm.
    - name: Update sudoers secure_path to include /usr/local/bin
      shell: |
        cp /etc/sudoers /etc/sudoers.{{ 10000 | random }}.bak
        sed -i "s!{{ sudoers_path.stdout }}!{{ sudoers_path.stdout }}:/usr/local/bin!g" /etc/sudoers     
      when: "'/usr/local/bin' not in sudoers_path.stdout"

    - name: Get /etc/dhcp/dhclient.conf
      shell: cat /etc/dhcp/dhclient.conf | grep domain-name-servers | grep {{ dns_server }}
      ignore_errors: true
      register: dhcp_nameserver_entry
      tags: ['dhcp']

    # This file will rebuild the /etc/resolv.conf file on reboot.
    # Leverage the private DNS lookup of the hosting provider
    - name: Override DNS entries
      shell: |
        cp /etc/dhcp/dhclient.conf /etc/dhcp/dhclient.conf.{{ 10000 | random }}.bak
        echo "supersede domain-name-servers {{ dns_server }};" >> /etc/dhcp/dhclient.conf
      register: dhcp_update

    - name: reboot hosts if required
      reboot:
        msg: "Reboot initiated by Ansible"
        connect_timeout: 5
        reboot_timeout: 600
        pre_reboot_delay: 0
        post_reboot_delay: 30
        test_command: whoami
      when: 
        - dhcp_update is not skipped 

    - name: Ping Nodes in hosts-skytap.yml after Reboot
      ping:
      when: dhcp_update is defined  

# install Helm on all hosts 
- name: Install helm on all hosts 
  become: true
  become_method: sudo
  become_user: root
  gather_facts: false
  hosts:  all

  tasks:
    - name: Check Helm
      shell: helm version
      register: helm_version
      ignore_errors: true
      tags: ['helm']

    - name: Download Helm
      get_url:
         url: https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
         dest: ./get_helm.sh
         mode: 700
      when: helm_version.rc != 0
      tags: [ helm ]
       
    - name: Install Helm
      shell: ./get_helm.sh
      when: "'not found' in helm_version.stderr"
      tags: ['helm']

# configure kubeconfig on cluster for {{ ansible_user }}
- name: Prepare Kubeconfig for {{ ansible_user }}
  become: true
  become_method: sudo
  gather_facts: false
  hosts:  "{{ groups['kube-master'][0] }}"
  any_errors_fatal: true
  tags: kubeconfig
  
  tasks:
    - name: Create ~/.kube directory
      file:
        path: ~/.kube
        state: directory
      become: false
      
    - name: copy config to {{ ansible_user }} home
      shell: "cp ~/.kube/config /home/{{ ansible_user }}/.kube/"

    - name: change config ownership to {{ ansible_user }}
      shell: "chown {{ ansible_user }}:{{ ansible_user }} /home/{{ ansible_user }}/.kube/config"

# setup kubectl for local user and root by copying from the cluster
- name: Setup kubectl for '{{ ansible_user }}' user on installer node
  become: true
  become_user: root
  become_method: sudo
  gather_facts: true
  any_errors_fatal: true
  tags: kubeconfig
  hosts: "{{ groups['installer'][0] }}"
   
  tasks:
    - name: Install jq 
      apt:
        name: jq
      when: ansible_distribution == "Ubuntu"

    # Install kubectl
    - name: Install kubectl
      raw: snap install kubectl --channel=1.18/stable --classic

    - name: Get local user home
      local_action: command echo $HOME
      register: local_home
      become: false
      tags: info

    - name: Create ~/.kube directory
      file:
        path: ~/.kube
        state: directory
      become: false

    - name: Transfer file from cluster to installer
      raw: scp -i {{ ansible_ssh_private_key_file }} {{ ansible_user }}@{{ groups['kube-master'][0] }}:/home/{{ ansible_user }}/.kube/config ~/.kube/config
      become: false

    # as root get config from {{ local_home.stdout }}
    - name: Create directory
      file:
        path: ~/.kube
        state: directory

    - name: copy config to root home
      shell: "cp {{ local_home.stdout }}/.kube/config ~/.kube"

    # as local user, test kubectl config
    - name: check kubectl connectivity from installer node to cluster
      shell: "kubectl get nodes"
      register: nodes
      become: false

    - name: show output from command --> kubectl get nodes
      debug:
        msg: "{{ nodes.stdout_lines }}"

    - name: check kubectl connectivity from installer node to cluster
      shell: "kubectl get pods -n kube-system"
      register: pods
      become: false

    - name: show output from command --> kubectl get pods -n kube-system
      debug:
        msg: "{{ pods.stdout_lines }}"

# install docker locally
- name: install Docker
  hosts: "{{ groups['installer'][0] }}"
  become: true
  become_method: sudo
  become_user: root
  gather_facts: true
  any_errors_fatal: true
  tags: docker
  vars:
    ansible_python_interpreter: /usr/bin/python3
    docker_home: /docker/registry
    
    tasks:
    - name: create group for non-root docker
      shell: |
        groupadd docker;
        usermod {{ local_user }} -aG docker;

    - name: check for docker
      shell: which docker
      ignore_errors: true
      register: docker_exists
      
    - name: install docker
      apt: 
        name: docker.io
      when: docker_exists is failed 

    - name: stop docker if running
      shell: systemctl stop docker
      ignore_errors: true
      when: docker_exists is defined 

    - name: create directory
      ignore_errors: true
      file:
        path: "{{ item }}"
        state: directory
      with_items:
        - /etc/docker
      when: docker_exists is failed

    - name: create docker daemon.json and data root
      shell: |
         echo '{ "data-root" : "{{ docker_home }}" }' > /etc/docker/daemon.json
         mv /var/lib/docker /docker
      tags: [install, docker]
      when: docker_exists is failed 

    - name: non-root docker directories
      shell: |
        chgrp docker "{{ item }}"
        chmod 775 "{{ item }}"
      with_items:
        - /etc/docker
        - /etc/docker/daemon.json
        - /docker/registry
      when: docker_exists is failed 

    - name: start docker
      shell: systemctl start docker

    - name: check root change
      shell: docker info | grep Root
      register: docker_root
      failed_when: '"{{ docker_home }}" not in docker_root.stdout'

- name: configure Docker Registry
  hosts: "{{ groups['installer'][0] }}"
  become: false
  gather_facts: true
  tags: [registry, continue]
  vars:
    ansible_python_interpreter: /usr/bin/python3
  # Only when the registry requested is the installer node will we install this.
  # Otherwise, we assume the registry is external and already defined
  # 
  any_errors_fatal: true
  tasks:
    - block:
      - name: Debug ID getting new group role
        shell: |
          # 'installer' user does not retrieve docker group while logged in
          # you will need to manually add the user to the docker group and re-run.
          newgrp docker
          id
        register: output

      - debug:
          msg: 
            - "{{ output.stdout_lines|list }}"
            - "To get docker group rights, you will need to logout and back in to continue."
            - "the gnome UI will retain your rights so you need to run: loginctl -terminate-user {{ local_user }}"
            - "or run the command: id to check which groups the user belongs too"
            - "then the command: sudo su - installer"
            - "then the command: id to check it belongs to docker group"
            - "When you re-run add the optional tag '-t continue' and you should see the docker group added on the second run."  
        failed_when: '"(docker)" not in output.stdout_lines|string'
      when: '"{{ master_node_for_registry }}" == "{{ groups[''installer''][0] }}"'

- name: configure Registry
  hosts: "{{ groups['installer'][0] }}"
  become: false
  gather_facts: true
  tags: [never, continue]
  vars:
    ansible_python_interpreter: /usr/bin/python3

  #  @TODO might want to add a check to confirm openssl 1.1.1 or the '-addext option will fail'
  tasks:
    - block:
      - name: create local certificate
        shell: |
          cd ~{{ local_user }};
          mkdir -p certs;
          # openssl 1.1.1
          openssl req -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key -x509 -days 365 -out certs/domain.crt -subj "/CN=dockerhost" -addext "subjectAltName=DNS:{{ groups['installer'][0] }}"
          # openssl req -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key -x509 -days 365 -out certs/domain.crt -subj "/CN={{ groups['installer'][0] }}"
        tags: [certs]

      - name: allow insecure registry
        shell: |
          sed -i "s! }!, \"insecure-registries\" : [\"{{ groups['installer'][0] }}:5000\"]}!1" /etc/docker/daemon.json

      - name: pip docker
        pip: 
          name: docker

      - name: build registry service
        docker_container:
          name: registry
          image: registry:2
          state: started
          detach: yes
          restart: yes
          restart_policy: unless-stopped
          #entrypoint: sleep 3000
          ports:
          - "5000:5000"
          volumes:
          - "/home/{{ local_user }}/certs:/certs"
          env:
              REGISTRY_HTTP_ADDR: "0.0.0.0:5000"
              REGISTRY_HTTP_TLS_CERTIFICATE: "/certs/domain.crt"
              REGISTRY_HTTP_TLS_KEY: "/certs/domain.key"       

      # do this manually since this restarts the running docker process on the installer node
      - name: restart docker for registry
        shell: systemctl restart docker
        become: true

      - name: check registry change
        shell: |
          docker info | grep -i -a1 Insecure 
        register: docker_insecure
        failed_when: '"{{ groups[''installer''][0] }}" not in docker_insecure.stdout'
      when: '"{{ master_node_for_registry }}" == "{{ groups[''installer''][0] }}"'

# Allow the nodes of the cluster to access the regsitry by certificate
- name: create Registry Cert path on cluster
  hosts: "{{ groups['kube-node']}}"
  become: true
  become_method: sudo
  become_user: root
  gather_facts: true
  tags: [never, continue]
  # Only when the registry requested is the installer node will we install this.
  # Otherwise, we assume the registry is external and already defined 
  any_errors_fatal: true
  tasks:
    - block:
      - name: create Docker cert directory on cluster
        shell: "mkdir -p /etc/docker/certs.d/{{ groups['installer'][0] }}:5000"

      - name: set ownership of cluster Docker cert directory to installer
        shell: "chown {{ ansible_user }}:{{ ansible_user }} /etc/docker/certs.d/{{ groups['installer'][0] }}:5000"
      when: '"{{ master_node_for_registry }}" == "{{ groups[''installer''][0] }}"'

- name: copy Docker cert from installer node to cluster
  become: false
  gather_facts: false
  hosts:  "{{ groups['kube-node'] }}" 
  any_errors_fatal: true
  tags: [never, continue]
  vars:
    ansible_python_interpreter: /usr/bin/python2

  # Only when the registry requested is the installer node will we install this.
  # Otherwise, we assume the registry is external and already defined
  tasks:
    - block:
      - name: write the clusters host key to known hosts
        connection: local
        shell: "ssh-keyscan -H {{ inventory_hostname }} >> ~/.ssh/known_hosts"

      - name: transfer file from installer to cluster
        shell: scp -i {{ ansible_ssh_private_key_file }} /home/{{ local_user }}/certs/domain.crt {{ansible_user }}@{{ inventory_hostname }}:/etc/docker/certs.d/{{ groups['installer'][0] }}:5000
        delegate_to: "{{ groups['installer'][0] }}"
      
      - name: verify new cert directory on cluster
        shell: | 
          cd /etc/docker/certs.d/{{ groups['installer'][0] }}:5000;
          pwd;
          ls -l;
        register: cert_path

      - name: show filepath for new docker cert
        debug:
          msg: "{{ cert_path.stdout_lines }}"
        when: '"{{ master_node_for_registry }}" == "{{ groups[''installer''][0] }}"'

- name: install Storage Class (if needed)
  hosts: "{{ groups['installer'][0] }}"
  become: false
  gather_facts: true
  tags: [never, continue, extras]
  vars:
    ansible_python_interpreter: /usr/bin/python3
    
  tasks:
    - name: preReq Yq
      shell: wget https://github.com/mikefarah/yq/releases/download/v4.2.0/yq_linux_amd64.tar.gz -O - | tar xz && mv yq_linux_amd64 /usr/bin/yq
      become: true

    - name: Check storage class
      shell: kubectl get sc -A | grep "(default)"
      register: sc
      ignore_errors: true
    
    - name: install openebs default storage (if needed)
      shell: "{{ item }}"
      with_items:
        - kubectl create ns openebs
        - helm repo add openebs https://openebs.github.io/charts
        - helm repo update
        - helm install --namespace openebs openebs openebs/openebs
      when: "'default' not in sc.stdout"

    - name: make openebs the default storageclass
      shell: |
        kubectl patch storageclass openebs-hostpath -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
      when: "'default' not in sc.stdout"

    - name: check Openebs storage class
      shell: kubectl get sc | grep default
      register: openebs
      failed_when: "'openebs-hostpath' not in openebs.stdout"
      when: "'default' not in sc.stdout"

# If you need to back up and try again this will remove software from the servers
# Note: we do not attempt to unedit files we may have added content to
- name: Cluster Hosts Reset
  hosts: "{{ groups['kube-node'] }}"
  become: true
  become_method: sudo
  become_user: root
  gather_facts: true
  tags: [reset, never]
  tasks:
    - name:  rm helm
      ignore_errors: true
      shell:
        cmd: "{{ item }}"
      with_items: 
      - rm ~{{ ansible_user }}/get_helm.sh;
      - rm /usr/local/bin/helm;
      - rm -rf /etc/docker/certs.d/{{ groups['installer'][0] }} 

- name: Installer Hosts Reset
  hosts: "{{ groups['installer'][0] }}"
  become: true
  become_method: sudo
  become_user: root
  gather_facts: true
  tags: [reset, never]
  tasks:
    - name:  rm docker and kubectl
      ignore_errors: true
      shell:
        cmd: "{{ item }}"
      with_items:
      - snap remove kubectl;
      - docker rm -f registry;
      - systemctl stop docker;
      - apt remove -y docker.io;
      - rm -rf /etc/docker
      - rm -rf /var/lib/docker
      - rm -rf /docker/registry
      - rm -rf ~/.kube;
      - rm -rf ~installer/.kube;
      - groupdel docker