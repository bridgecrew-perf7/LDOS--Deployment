- name: Install Dataflow Designer over running Foundry cluster
  become: false
  gather_facts: true
  hosts: "{{ groups['installer'][0] }}"
  vars:
    logs_dir: /installers/logs
    env_file: /installers/LumadaDataOpsSuite-1.1.1/installer/env.properties
    designer_home: /installers/LumadaDataOpsSuite-1.1.1
    designer_archive: .tgz
    installer_archive: Lumada-DataOps-Suite-installer-1.1.1.zip
    
    ansible_python_interpreter: /usr/bin/python3
    
  tasks:
    # Checks for the logs directory
    - name: Check if 'logs' directory exists
      stat:
        path: "{{ logs_dir }}"
      register: logs
      tags: [info] 

    - debug: 
        msg: "logs directory exists: {{ logs.stat.exists }}"
      tags: [info]           

    # Create a logs directory - if required
    - name: Creates 'logs' Directory
      file:
       path: "{{ logs_dir }}"
       state: directory
      when: logs.stat.exists == false
      tags: [info, install]
    
    # Check for Dataflow Designer directory
    - name: Check if the 'Dataflow Designer' directory exists
      stat:
        path: "{{ designer_home }}"
      register: designer
      tags: [info]

    - debug: 
        msg: "Dataflow Designer directory exists: {{ designer.stat.exists }}"
      tags: [info]

    # Creates Dataflow Designer install directory - if required
    - name: Creates 'Dataflow Designer' directory
      file:
        path: "{{ designer_home }}"
        state: directory
      when: designer.stat.exists == false
      tags: [info]

    # Unarchive Dataflow Designer 0.7.0
    - name: Unarchive {{ designer_archive }}
      unarchive:
        src: "{{ designer_home }}/{{ designer_archive }}"
        dest: "{{ designer_home }}"
        creates: "{{ designer_home }}/lumada-dataops-suite"
      tags: [unpack]  

    # Unarchive Lumada-DataOps-Suite-Installer-1.1.1.zip
    - name: Unarchive {{ installer_archive }}
      unarchive:
        src: "{{ ldos_home }}/{{ installer_archive }}"
        dest: "{{ ldos_home }}"
        creates: "{{ ldos_home }}/installer"
      tags: [unpack]  

    # Prepare env.properties
    - name: get secret for solution-control-plane-sso-client
      shell: 'kubectl get secrets keycloak-client-secret-solution-control-plane-sso-client -n hitachi-solutions --template=\{\{.data.CLIENT_SECRET\}\} | base64 --decode'
      register: scp_sso_client_secret
      tags: [env]

    # Get the 'foundry' password 
    - name: get foundry password
      shell: "kubectl get keycloakusers -n hitachi-solutions keycloak-user -o jsonpath='{.spec.user.credentials[0].value}'"
      register: foundry
      tags: [env]
    
    # Replace the variables in env.properties
    - name: Replace variables in {{ env_file }} before install
      shell: | 
        cp {{ env_file }} {{ logs_dir }}/{{ env_file | basename }}.{{ 10000 | random }}.bak
        sed -i 's|^hostname=*$|hostname={{ apiserver_loadbalancer_domain_name }}|1' {{ env_file }}
        sed -i 's|^registry=*$|registry={{ master_node_for_registry }}:{{ master_node_for_registry_port }}|1' {{ env_file }}
        sed -i 's|^foundry_client_secret=*$|foundry_client_secret={{ scp_sso_client_secret.stdout }}|1' {{ env_file }}
        sed -i 's|^username=*$|username=foundry|1' {{ env_file }}
        sed -i 's|^password=*$|password={{ foundry.stdout }}|1' {{ env_file }}
        sed -i 's|^volume_host=*$|volume_host={{ nfs_host }}|1' {{ env_file }}
        sed -i 's|^volume_path=*$|volume_path={{ nfs_path }}|1' {{ env_file }}
      tags: [env]

    # Update Hostnames in HELM charts. Hostname is defined with {{placeholder}} in Helm Charts.  
    # Careful as once defined there's no going back..!
    - name: Update Dataflow Designer Hostname
      shell: 
        chdir: "{{ designer_home }}"
        cmd: "./installer/update-hostname.sh -c=lumada-dataops-suite/charts -h={{ apiserver_loadbalancer_domain_name }} -D true 2>&1 | tee -a {{ logs_dir }}/update-host-name.log"
      async: 2500
      poll: 30
      register: ret
      failed_when: "ret.rc > 0 or 'no such file' in ret.stdout"
      tags: [update_hostname, designer]

    # Upload Dataflow Designer 0.7.0 solutions to Registry.
    - name: Upload Dataflow Designer Solutions to Foundry
      shell: 
        chdir: "{{ designer_home }}/lumada-dataops-suite"
        cmd: "./control-plane/bin/upload-solutions.sh -C ./charts/ -I ./images/ -D true 2>&1 | tee -a {{ logs_dir }}/upload-ldos-solution.log"
      async: 2500
      poll: 30
      register: ret
      failed_when: "ret.rc > 0 or 'no such file' in ret.stdout"
      tags: [install, upload_designer]

    # Install Dataflow Designer 0.7.0
    - name: Install Dataflow Designer 0.7.0
      shell: 
        chdir: "{{ designer_home }}/installer"
        cmd: "./install.sh 2>&1 | tee -a {{ logs_dir }}/install-dataflow-designer.log"
      async: 1000
      poll: 30
      register: ret
      failed_when: "ret.rc > 0 or 'no such file' in ret.stdout"
      tags: [designer]

    # Check for Running and Complete Pods
    - name: Confirm Install
      shell: "kubectl get pods -n hitachi-solutions | grep -i -e '[RC][uo][nm][np][il][ne]' | wc -l"
      register: designer_pods
      tags: [designer, info]

    #Check for Non-Running or Failing Pods
    - name: Confirm application status (URL or pods?)
      shell: "kubectl get pods -n hitachi-solutions | grep -iv -e '[RC][uo][nm][np][il][ne]'"
      register: results
      tags: [designer, info]

